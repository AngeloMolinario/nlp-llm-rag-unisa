Encoder only transformers | transformer, neural network, encoder only, sequence classification, sequence representation, encoder-only |
Transformers are a neural network architecture introduced in 2017 and are widely used in Natural Language Processing (NLP). Although the full architecture includes both an encoder and a decoder, in some tasks, it is sufficient to use only the encoder part. This approach, known as encoder-only transformers, is particularly useful for tasks that involve transforming a sequence into another sequence of the same length or for sequence classification. The original application of the Transformer model was in translation, which typically requires an encoder-decoder architecture. However, for tasks where the transformation results in a sequence of the same length or a single output value, such as text classification or sentiment analysis, only the encoder is needed. In such cases, the output of the encoder, denoted as z1,...,zt, serves as the final representation of the sequence. When the goal is to map a sequence to a single value, a special token is often used as the first input element, and the corresponding encoder output z1 is treated as the final representation, upon which the loss function is computed.

Architecture of Encoder-Only Transformers | encoder only, encoder-only, achitecture, structure, self-attention, bidirectional self-attention, context comprehension |
When processing textual sequences, encoder-only transformers use a contextual representation of each token through bidirectional self-attention. This means that each word in a text can be interpreted based on the surrounding words, thereby improving context comprehension. This is particularly useful for tasks such as sentiment analysis, entity recognition, and text classification.

BERT and Its Structure | bert, bidirectional, transformer, encoder, encoder only, encoder-only|
One of the most well-known encoder-only transformers is BERT (Bidirectional Encoder Representations from Transformers), introduced by Google Research in 2018. Unlike previous models, BERT exclusively uses the encoder component of the Transformer and is pre-trained to use bidirectional context, meaning it considers both the preceding and succeeding context of a word. There are two main variants of BERT: the Base version, which includes 12 stacked encoder blocks with 110 million parameters, and the Large version, which consists of 24 stacked encoder blocks with 340 million parameters.

Tokenization and Input Encoding in BERT | tokenization, wordpiece, special token, bert input, cls, sep, input encod|
BERT uses WordPiece tokenization, which segments text into subwords. This approach allows BERT to handle new, rare, and even misspelled words by breaking them down into recognizable units. The WordPiece tokenizer builds a vocabulary of common words and subword units so that rare or out-of-vocabulary words can be broken down into familiar subwords. Sentences are splitted into tokens based on whitespace, punctuation, and common prefixes. BERT also use special tokens, [CLS] and [SEP]. [CLS] is a classification token that is added at the beginning of each input sequence. [SEP] is a separation token used to mark the end of a sentence or to separate pairs of sentences within an input sequence. Once tokenized, each token is mapped to an ID from BERT’s vocabulary, which serves as input to the model. Within BERT, the [CLS] token, prepended to the beginning of every input sequence, plays a crucial role. This "classification token" serves as a summary representation of the entire input. After BERT processes the input, the final hidden state of the [CLS] token becomes a condensed, context-aware embedding for the complete sentence or sequence of sentences. This embedding can then be passed to additional layers, such as a classifier, for specific tasks. The [CLS] token's usage varies slightly depending on whether the task involves single-sentence or sentence-pair classification. For single-sentence classification, the final hidden state of the [CLS] token is directly passed to a classifier layer to generate predictions. A typical example is sentiment analysis, where the classifier might predict "positive" or "negative" sentiment based on the [CLS] embedding. For tasks involving two sentences, BERT tokenizes them in the following structure: [CLS] Sentence A [SEP] Sentence B [SEP]. In these cases, the [CLS] token's final hidden state captures the relationship between the two sentences, making it suitable for tasks like entailment detection or similarity scoring. This structured input allows BERT to effectively process and understand text in a way that preserves both local and global linguistic context. This WordPiece tokenization provides several key advantages: it improves efficiency by reducing vocabulary size while maintaining expressive power; it enables BERT to handle unseen words by breaking them into recognizable subword units; and it enhances BERT's language understanding by allowing it to learn meaningful subword components during pretraining, ultimately helping it capture complex linguistic patterns.

Training BERT | bert, bert training, masked language modeling, mlm, next sentence prediction, nsp, pre-training, self-supervised learning, token masking, language modeling |
During BERT's pre-training phase, two self-supervised learning strategies are employed, Masked Language Modeling (MLM) and Next Sentence Prediction (NSP).  For MLM, a percentage of input tokens are randomly masked (replaced with a [MASK] token), and the model's objective is to predict the actual token IDs for these masked tokens.  Specifically, 15% of the WordPiece tokens are selected for masking during training. In addition to this MLM objective, BERT also undergoes secondary training for Next Sentence Prediction (NSP).  Many downstream tasks, such as question answering (QA), rely on understanding the relationship between two sentences, a capability that cannot be achieved through language modeling alone.  For the NSP training phase, a simple binarized task is created.  Pairs of sentences, A and B, are selected for each training sample and labeled as either IsNext or NotNext.  Half of the pre-training samples consist of sentence pairs where sentence B genuinely follows sentence A in the corpus.  For the other half, sentence B is chosen randomly. This straightforward approach demonstrates that effective solutions don't always require complex designs.  The training dataset for BERT is extensive, comprising a corpus of publicly available books and the English Wikipedia, exceeding 3 billion words in total.

Fine-Tuning BERT | bert, fine-tuning, cross-entropy loss, loss, cross entropy, gradient-based optimization, sentiment analysis, named entity recognition, ner |
After the pre-training phase, BERT's true power is unleashed through fine-tuning.  The output from BERT's encoder isn't directly used for specific tasks; instead, it's passed to an additional layer or layers tailored to the problem at hand. For classification, this usually involves minimizing the difference between BERT's predictions and the actual labels using a cross-entropy loss and gradient-based optimization. The added layers are trained specifically for the task, while the pre-trained BERT parameters might be adjusted slightly or kept fixed, depending on the situation. The magic of BERT lies in its pre-training on a massive amount of general language data. This gives it a strong foundation, which is then refined through fine-tuning on smaller, task-specific datasets. A crucial element of this process is the [CLS] token, this is because its final embedding is the key input for the downstream task and is specifically trained to represent the input sequence in the way most relevant for that task. This fine-tuning approach makes BERT incredibly versatile because it allows to solve tasks like sentiment analysis, where you want to classify text as positive, negative, or neutral. Named entity recognition, where the goal is to identify and categorize named entities like people, dates, and organizations within a text. The good things is that only minimal adjustments are needed for each task. Just a few extra layers are typically enough, making BERT remarkably efficient and adaptable for a wide spectrum of NLP applications.

BERT: Strenghts and Limitations | bert, bert strength, bert limitation|
BERT boasts several key strengths.  Its bidirectional contextual understanding provides richer and more accurate representations of language, capturing nuances that unidirectional models might miss.  The pre-training process allows for remarkable flexibility in transfer learning, enabling easy adaptation to a diverse range of NLP tasks.  This adaptability is replete in BERT's consistently high performance on benchmark datasets, where it frequently ranks at or near the top on datasets like SQuAD (for question answering) and GLUE (for general language understanding). However, BERT also has limitations.  Its large model size leads to high computational and memory requirements, which can make deployment challenging, especially in resource-constrained environments.  The pre-training phase itself demands extensive computational resources, particularly for larger versions of BERT like BERT-Large. Finally, while fine-tuning is generally easier than training from scratch, it still requires labeled data for new tasks and can be time-intensive, especially for large datasets.

BERT variants: RoBERTa, ALBERT, DistilBERT, TinyBERT, ELECTRA | bert, bert variant, roberta, albert, distilbert, tinybert, electra |
RoBERTa, short for Robustly Optimized BERT Approach, was developed by Facebook AI in 2019.   It builds upon the foundation of BERT but introduces several key modifications. First, RoBERTa is trained on a significantly larger corpus of data, boasting 200 billion words compared to BERT's original training set.Second, and perhaps surprisingly, the Next Sentence Prediction (NSP) task, used in BERT's pre-training, is removed in RoBERTa. Research indicated that eliminating NSP actually improves overall performance. Third, RoBERTa undergoes longer training, meaning more iterations, and employs larger batch sizes for more robust language modeling. Finally, RoBERTa utilizes dynamic masking. Instead of applying the same mask throughout the training process, the masking is applied dynamically, meaning different masks are used for each epoch. This dynamic masking leads to better generalization and improved performance. As a result of these enhancements, RoBERTa consistently outperforms BERT on various NLP benchmarks, especially in tasks demanding nuanced language understanding.
ALBERT, or A Lite BERT, developed by Google Research in 2019, offers a more efficient alternative to BERT. It achieves this efficiency through several key architectural changes. First, ALBERT employs factorized embedding parameterization, a technique that reduces the model size. Second, it utilizes cross-layer parameter sharing, where weights are shared across different layers to further decrease the overall number of parameters. Finally, ALBERT replaces the Next Sentence Prediction (NSP) task with Sentence Order Prediction (SOP). SOP proves to be more effective at capturing inter-sentence coherence, the relationships between sentences in a text. The result of these modifications is that ALBERT achieves comparable performance to BERT-Large while using significantly fewer parameters, making it much more memory-efficient. This smaller footprint translates to faster and lighter processing, making ALBERT ideal for applications where computational resources are limited, such as mobile devices or embedded systems.
DistilBERT, short for Distilled BERT, is a creation of Hugging Face designed for efficiency. Its primary difference from BERT lies in its use of model distillation. This technique allows DistilBERT to reduce BERT's size by approximately 40% while remarkably retaining 97% of its language understanding capabilities. DistilBERT achieves this compression partly by having fewer layers,6 instead of the 12 found in BERT-Base, but these layers are carefully optimized to maintain similar performance. The result is faster inference and lower memory usage, making DistilBERT particularly well-suited for real-time applications where speed and resource constraints are critical. DistilBERT has become widely popular for lightweight applications that demand a smaller and faster model without significant sacrifices in accuracy.
TinyBERT, developed by Huawei, pushes the boundaries of BERT efficiency even further. Its key distinguishing feature is its use of a two-step knowledge distillation process. TinyBERT distills knowledge from BERT both during the pre-training and fine-tuning stages, maximizing efficiency gains. This dual distillation, combined with architectural optimizations, makes TinyBERT even smaller and faster than DistilBERT, specifically targeting mobile and edge devices where resources are severely limited. Despite its ultra-compact size, TinyBERT maintains accuracy close to that of BERT on various NLP tasks, especially when fine-tuned with task-specific data. This makes TinyBERT an ultra-compact version of BERT well-suited for extremely resource-constrained environments.
ELECTRA, which stands for Efficiently Learning an Encoder that Classifies Token Replacements Accurately, is a creation of Google Research that offers a different approach to pre-training language models. Its key distinction from BERT lies in its training strategy. Instead of relying on masked language modeling (MLM), ELECTRA employs a generator-discriminator setup.  In this setup, the model learns to identify replaced tokens within the text, similar to how a discriminator in a GAN would distinguish real from fake images. This approach proves to be remarkably efficient, allowing ELECTRA to learn with fewer computational resources and converge faster than BERT. Furthermore, ELECTRA often outperforms BERT on language understanding benchmarks, achieving higher performance with significantly less compute power. ELECTRA’s training efficiency and robust performance make it an attractive option for applications where computational resources are limited, offering a compelling balance between performance and efficiency.

BERT variants: SciBERT, BioBERT, ClinicalBERT, mBERT, others | bert, bert variant, scibert, biobert, clinicalbert, mbert, camembert, finbert, legalbert |
SciBERT is a specialized version of BERT tailored for applications involving scientific literature. This makes it particularly well-suited for academic and research-oriented NLP tasks. Its strength lies in its domain-specific pre-training. SciBERT is trained on a large corpus of scientific papers spanning various domains, including biomedical and computer science. This specialized training is coupled with a vocabulary tailored to the language of science, which better represents scientific terms and jargon that might be absent or underrepresented in general-purpose vocabularies. As a result, SciBERT significantly outperforms BERT on tasks specific to scientific NLP, such as scientific text classification, named entity recognition (NER), and relation extraction.
BioBERT has become a widely adopted tool in the biomedical research field, playing a crucial role in information extraction and knowledge discovery from medical literature. Its effectiveness stems from its specialized training. BioBERT is pre-trained on a substantial biomedical text corpus, which includes PubMed abstracts and full-text articles from PMC. This domain-specific training enables BioBERT to achieve enhanced performance on biomedical tasks. It excels at tasks like medical named entity recognition (NER), relation extraction, and question answering within the healthcare domain, making it a valuable asset for researchers and practitioners in the biomedical field.
ClinicalBERT is specifically designed for hospitals and healthcare providers who need to analyze patient records, predict health outcomes, or assist in clinical decision-making. Its focus is squarely on the healthcare domain, tailored for processing clinical notes and tackling healthcare-related NLP tasks. A key element of ClinicalBERT's effectiveness is its training. It is pre-trained on the MIMIC-III database, a widely used repository of clinical records. This specialized pre-training on real-world clinical data makes ClinicalBERT particularly useful for healthcare analytics, enabling it to better understand the nuances of medical language and improve performance on tasks relevant to patient care.
mBERT, developed by Google, is designed to support NLP tasks across a wide range of languages. This multilingual capability enables the development of global applications and facilitates cross-lingual transfer learning.  Trained on a massive dataset spanning 104 languages, mBERT can handle multilingual text without the need for separate models for each individual language. This is achieved through its ability to learn language-agnostic representations, meaning it captures underlying linguistic structures common across languages.  This language-agnosticism makes mBERT capable of zero-shot cross-lingual transfer, allowing it to perform tasks in languages it hasn't explicitly been trained on. This makes it particularly well-suited for translation and other cross-lingual understanding tasks.
Other variants beyond the core BERT model: several specialized versions have been developed to cater to specific domains and languages. CamemBERT, for instance, is a BERT model specifically focused on the French language, optimized for the nuances and characteristics of French text. FinBERT is tailored for financial text analysis, enabling better understanding and processing of financial documents and information. Similarly, LegalBERT is trained on a large corpus of legal documents, enhancing its performance within the legal domain. The influence of BERT's Transformer-based pre-training has even extended beyond natural language processing. It has inspired similar architectures in computer vision, leading to the development of vision Transformers, Swin Transformers, and Masked Autoencoders (MAE). These models adapt the Transformer architecture and pre-training strategies to image data, demonstrating the broad applicability of the underlying principles pioneered by BERT.