Term Frequency (TF) | term frequency, document, text representation |
Term Frequency (TF) measures how often a word appears in a document. It is a foundational concept in text representation and is often normalized to account for varying document lengths. 

Limitations of TF | normalization, term frequency |
TF does not account for document length. For example, the word "dog" may appear 3 times in a 30-word email and 100 times in a 580,000-word novel. Without normalization, the word "dog" would appear disproportionately significant in the novel. 
Normalized TF addresses this by dividing the raw count of a word by the total number of words in the document. For instance, the normalized TF for "dog" in the email would be 3/30 = 0.1, while in the novel, it would be 100/580000 that is approximately 0.00017. Normalization is very useful in comparing documents.

Natural Language ToolKit (NLTK) Corpora | reuters, nltk, tokenization, text processing, tdm, term-document matrix |
The Natural Language Toolkit (NLTK) provides access to multiple text corpora, essential for training and evaluating Natural Language Processing (NLP) algorithms. Among these, the Reuters 21578 corpus is widely used for text classification tasks, containing thousands of news articles published by Reuters in 1986. These articles are categorized into 90 different topics, making the dataset a valuable resource for experimenting with NLP techniques.
To process this corpus, text tokenization is a crucial first step. Tokenization involves breaking text into individual words or tokens while filtering out irrelevant elements such as stopwords, punctuation, and whitespace. A common method for representing tokenized text is the Bag of Words (BoW) model, which captures word frequencies in a document without considering word order. Using this approach, term frequencies (TF) can be computed, highlighting the most relevant words in a given document.
Expanding the scope of analysis, processing multiple documents requires transforming raw text into a structured numerical format. This is achieved by creating a Term-Document Matrix (TDM), where rows represent individual documents, columns represent unique terms in the vocabulary, and values correspond to the frequency of each term within a document. Missing values in this matrix are typically replaced with zeros to maintain consistency across the dataset.
An important consideration when processing large text datasets is computational efficiency. Standard NLP libraries, such as spaCy, offer multi-step processing pipelines that include operations like Part-of-Speech (POS) tagging, lemmatization, and Named Entity Recognition (NER). While these operations provide rich linguistic insights, they are computationally expensive and unnecessary when the goal is simple tokenization. To optimize performance, disabling unnecessary steps in the spaCy pipeline can significantly reduce processing time, making large-scale text analysis more efficient.
By implementing these optimizations, a scalable text processing workflow can be established. The resulting structured term-document representation serves as a foundation for various NLP applications, including document similarity analysis, information retrieval, and search engine ranking algorithms.

Vector Space Model | vector space model, vector space model, compare, document, term-document matrix, tdm |
The Vector Space Model represents documents as vectors in a multidimensional space, where each dimension corresponds to a word. This model is widely used in NLP for tasks like document comparison and information retrieval.
It's built from a Term-Document Matrix (TDM) is a common representation, where rows represent documents, columns represent terms from the vocabulary, and elements can be TF, normalized TF, or other metrics.

Document Similarity | document similarity, compare, magnitude, direction, cosine similarity,  euclidean distance |
Document similarity is a key concept in NLP, often measured using Euclidean distance or cosine similarity. Euclidean distance measures the straight-line distance between two vectors but is sensitive to the magnitude of the vectors, making it less suitable for NLP tasks. Cosine similarity, on the other hand, measures the cosine of the angle between two vectors, focusing on their direction rather than magnitude. It assumes values between -1 and 1. It's more effective for comparing text representations, especially when using normalized vectors.
A cosine similarity of 1 means that the vectors point in the same direction across all dimensions. This suggests that documents use the same words in similar proportions, making it highly likely that they discuss the same topic or have similar content.
A cosine similarity of 0 occurs when the vectors are orthogonal, meaning that they share no common words. In this case, the documents are likely discussing completely different topics with no textual overlap.
A cosine similarity of -1 indicates that the vectors point in opposite directions in all dimensions. However, this situation is impossible with Term Frequency (TF) since word counts cannot be negative. A similarity score of -1 can occur in other word representations, where certain transformations or embeddings allow for negative vector values.

TF-IDF (Term Frequency-Inverse Document Frequency) | tf-idf, rare, importance, weight, frequency |
TF-IDF is a weighting scheme that combines Term Frequency (TF) and Inverse Document Frequency (IDF). It highlights words that are frequent in a document but rare in the corpus, making them significant for that document. IDF measures the importance of a word in relation to the entire corpus, increasing the weight of rare words and decreasing the weight of common words.
The TF-IDF formula is: TF(t,d) * IDF(t,D)
where TF(t,d) is the term frequency of term t in document d, and IDF(t,D) is the inverse document frequency of term t in corpus D. A high TF-IDF score indicates a term that is significant for a document, while a low score indicates a term that is either infrequent in the document or common across the corpus.
TF-IDF has multiple alternative weighting schemes, each designed to refine how term importance is measured in a document. The most basic approach, labeled as "None," assigns the raw term frequency directly, represented as w_ij = f_ij. The standard TF-IDF formula incorporates logarithmic scaling for both term frequency and inverse document frequency, expressed as w_ij = log(f_ij) * log(N / n_j). A variation known as TF-ICF, or Term Frequency-Inverse Category Frequency, follows a similar structure but replaces document frequency with category frequency, using the formula w_ij = log(f_ij) * log(N / f_j).
A more advanced probabilistic method, Okapi BM25, normalizes term frequency and applies an inverse document frequency adjustment, given by w_ij = (f_ij / (0.5 + 1.5 * (f_j / f̄_j) + f_ij)) * log((N - n_j + 0.5) / (f_ij + 0.5)). Another approach, ATC or Augmented Term Frequency with Cosine Normalization, adjusts term frequency based on the maximum frequency within a document and applies normalization over all terms, following the formula w_ij = ((0.5 + 0.5 * (f_ij / f_max)) * log(N / n_j)) / sqrt(sum_{i=1}^{n} [(0.5 + 0.5 * (f_ij / f_max)) * log(N / n_j)]^2).
Another weighting model, LTU, refines the logarithmic transformation by incorporating a modified term frequency scaling, given by w_ij = ((log(f_ij) + 1.0) * log(N / n_j)) / (0.8 + 0.2 * f_j / f̄). These different weighting methods help optimize information retrieval by adjusting the influence of term frequency and document frequency, enhancing the effectiveness of search algorithms.
Additional alternatives to TF-IDF rely on statistical techniques. Mutual Information, represented as w_ij = log(P(t_ij | c_j) / (P(t_ij) P(c_j))), measures how strongly a term is associated with a category compared to its overall distribution. A variation, Positive Mutual Information, ensures that the weight remains non-negative by taking the maximum of zero and the Mutual Information value, expressed as w_ij = max(0, MI). The T-Test measures the deviation of a term's probability from its expected distribution, using the formula w_ij = (P(t_ij | c_j) - P(t_ij) P(c_j)) / sqrt(P(t_ij) P(c_j)).
Several additional term weighting strategies refine frequency-based calculations. Lin98a computes term importance as the product of term frequency and inverse document frequency, expressed as w_ij = (f_ij * f) / (f_j * f_j). Another variation, Lin98b, applies a logarithmic transformation with a negative scaling factor, represented as w_ij = -1 * log(n_j / N). Gref94 modifies the TF-IDF approach by adjusting the logarithmic scale for both term and document frequency, given by w_ij = log(f_ij + 1) / log(n_j + 1).
Each of these alternative weighting schemes offers unique adjustments to standard TF-IDF, optimizing document retrieval for different types of corpora and classification tasks.

Zipf's Law | frequency, zipf, law, occurrence |
Zipf's Law observes that the frequency of a word is inversely proportional to its rank in a frequency table. This means that a small set of highly frequent words dominates language usage, while the majority of words are relatively rare. For example, the most common word appears approximately twice as often as the second most common word, three times as often as the third, and so on.
The Zipf's Law formula is: f(r) = K/r^a, where f(r) is the frequency of the word at rank r, K is a constant, r is the rank of the word, a determines the shape of the distribution which is approximately equals to 1.
In NLP, the use of logarithms in IDF mitigates the influence of rare words, resulting in a more uniform distribution of TF-IDF scores. This helps balance the importance of common and rare terms in text representation.

Building a Search Engine | search engine, search engine, tf-idf, cosine similarity |
TF-IDF matrices are the backbone of information retrieval systems, such as search engines. The process involves tokenizing all documents and creating a TF-IDF matrix. When a user inputs a query, the query is treated as a document, and its TF-IDF vector is calculated. The cosine similarity between the query vector and all document vectors is computed, and the documents with the highest similarity scores are returned as search results.
To optimize the process, real-world search engines use an inverted index, which maps each word in the vocabulary to all documents containing that word. This reduces the number of documents that need to be compared with the query, improving efficiency.