Encoder-Decoder Transformers |transformer, encoder-decoder, sequence-to-sequence, seq2seq, T5, translation, summarization|  
Encoder-Decoder Transformers are a class of neural networks designed for sequence-to-sequence tasks. These models consist of two main components: an encoder and a decoder. The encoder processes the input sequence and generates contextual embeddings, while the decoder uses these embeddings to autoregressively generate the output sequence. This architecture is particularly effective for tasks like machine translation, text summarization, and question answering, where the model must map an input sequence to an output sequence of varying lengths. An example of encoder-decoder model is T5.

T5 (Text-to-Text Transfer Transformer) |T5, encoder-decoder, Google Research, Text-to-Text, span-corruption, C4 dataset|  
T5, developed by Google Research, is a unified framework that treats all natural language processing tasks as text-to-text problems. Both the input and output are represented as text strings, regardless of the task. T5 is based on an encoder-decoder architecture and is pre-trained using a denoising objective called **span-corruption**. In this approach, random spans of text in the input sequence are masked with unique tokens (e.g., <extra_id_0>), and the model is trained to predict the original masked spans. This encourages the model to learn both global context and local coherence, making it highly versatile for downstream tasks. T5 is pre-trained on the **C4 dataset** (Colossal Clean Crawled Corpus), which consists of approximately 750 GB of cleaned text derived from Common Crawl. The dataset is rigorously filtered to remove spam, duplicates, and low-quality content, ensuring diverse and high-quality training data.  

T5 Architecture Variants |T5, encoder-decoder, model size, parameters|  
T5 comes in several sizes to accommodate different computational resources and performance requirements. The smallest version, T5-Small, has 6 encoder and decoder blocks, 8 attention heads, and 512-dimensional embeddings. The largest version, T5-XXL, features 24 encoder and decoder blocks, 64 attention heads, and 4096-dimensional embeddings.  

T5 Input Encoding |T5, tokenizer, SentencePiece, unigram, subword, special tokens|  
T5 uses a **SentencePiece tokenizer** with a fixed vocabulary of 32,000 tokens. This tokenizer employs a subword-based approach, balancing character-level and word-level tokenization. Subword tokenization is effective for handling rare words and unseen combinations, as it breaks down complex words into smaller, meaningful units.  
The vocabulary includes special tokens such as <pad> for padding, <eos> for marking the end of a sequence, and <sep> for separating different parts of the input. Additionally, T5 uses task-specific prefixes (e.g., "translate English to German:") to guide the model in performing different tasks.  

T5 Pre-Training |T5, span-corruption, denoising, Adafactor, C4 dataset|  
During pre-training, T5 is trained to predict masked spans of text using a cross-entropy loss function. The span-corruption objective involves replacing random spans of text in the input sequence with unique tokens (e.g., <extra_id_0>) and training the model to predict the original masked spans in sequential order. This approach forces the model to learn both the global context of the sentence and the local relationships between tokens, ensuring that the generated text is fluent and coherent.  
The model is optimized using the **Adafactor optimizer**, which is designed to be memory-efficient and well-suited for large-scale training. The learning rate is adjusted using a warm-up phase followed by an inverse square root decay, which helps stabilize training and improve convergence.  

T5 Fine-Tuning |T5, fine-tuning, text-to-text, summarization, translation|  
Fine-tuning T5 involves adapting the pre-trained model to specific downstream tasks by continuing the text-to-text paradigm. For example, in summarization tasks, the input is formatted as "summarize: <document>", and the output is the generated summary. Similarly, for translation tasks, the input might be "translate English to French: <text>", and the output is the translated text. This unified approach allows T5 to handle a wide range of tasks without requiring significant architectural changes.  

Popular T5 Variants |T5 variants, mT5, Flan-T5, ByT5, UL2, Multimodal T5|  
Several variants of T5 have been developed to address specific needs and challenges. **mT5** extends T5's capabilities to multiple languages, supporting 101 languages and demonstrating strong performance in cross-lingual tasks. **Flan-T5** is a fine-tuned version of T5 that has been trained on a diverse set of instruction-response pairs, enabling it to perform well in zero-shot and few-shot learning scenarios.  
**ByT5** processes text at the byte level, eliminating the need for tokenization and making it robust for handling noisy or unstructured text. **UL2** (Unified Language Learning) combines multiple pretraining objectives, including unidirectional, bidirectional, and sequence-to-sequence tasks, to achieve state-of-the-art performance across a variety of benchmarks.  
For tasks involving both text and images, **Multimodal T5** integrates additional modules for processing visual data, enabling applications such as image captioning and visual question answering. Finally, **Efficient T5** variants, such as T5-Small and DistilT5, are optimized for resource-constrained environments, offering faster inference and lower memory requirements at the cost of some performance.  