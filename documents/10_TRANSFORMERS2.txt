Multi-Head Attention | multi-head attention, self-attention, independent attention, weight matrix, context representation, syntactic dependency, semantic dependency, word meaning, attention head, concatenation |
A single attention mechanism might not be sufficient to capture all aspects of a word’s meaning in different contexts. Multi-head attention extends self-attention by computing multiple independent attention functions in parallel, each with its own set of learned weights. These independent attention heads allow the model to capture diverse relationships within the data, such as syntactic and semantic dependencies. The results from all heads are concatenated and passed through a final weight matrix to combine them into a unified representation. This approach enhances the model’s ability to understand words in varying contexts, preventing a single attention head from dominating the learning process.

Add & Norm | add & norm, residual connection, layer normalization, gradient flow, deep network, vanishing gradient, activation mean, regularization, stabilization, convergence |
To stabilize training and facilitate gradient flow in deep networks, residual connections and layer normalization are applied after each multi-head attention layer. Residual connections help mitigate the problem of vanishing gradients by allowing gradients to bypass certain layers, making it possible to train deeper models. Layer normalization ensures that activations have a mean of zero and a standard deviation of one, improving stability and regularization. The combination of these techniques enables the model to converge faster and generalize better to unseen data.

Feed Forward | feed forward, position-wise, linear layer, non-linearity, relu, gelu, word transformation, token processing, contextual dependency |
After the self-attention mechanism, a position-wise feed-forward network is applied to each token separately. This network consists of two linear layers with a non-linearity (such as ReLU or GELU) in between. The feed-forward network introduces additional transformations that help capture complex relationships between words, ensuring that the model learns rich representations beyond what is captured by attention alone. Since this step is applied independently to each token, it does not interfere with the contextual dependencies established by the attention layers.

Transformer’s Encoder | transformer encoder, self-attention, feed-forward network, residual connection, layer normalization, contextual representation, positional encoding, stacked layers, word order |
The Transformer encoder consists of multiple layers of self-attention and feed-forward networks, each with residual connections and layer normalization. The encoder is responsible for computing a contextual representation of the input sequence. Since self-attention is order-agnostic, positional encoding is added to the input embeddings to introduce word order information. Each encoder block receives the same input structure and produces an output of the same dimension, allowing multiple encoder layers to be stacked to refine the representation further.

Transformer’s Decoder | transformer decoder, encoder output, masked self-attention, sequence generation, autoregressive, decoder block, encoder-decoder attention, prediction |
The decoder generates an output sequence step by step while attending to both the encoder outputs and previously generated tokens. It follows a structure similar to the encoder but includes additional mechanisms, such as masked self-attention, to prevent information leakage from future tokens. This ensures that the model generates each token based only on past tokens. Each decoder block consists of masked self-attention, encoder-decoder attention, and a feed-forward network. The encoder-decoder attention module allows the decoder to focus on relevant parts of the input sequence by attending to the encoder outputs. The final decoder layer produces a contextual representation of the target sequence, which is then passed to the output layer for prediction.

Masked Multi-Head Attention | masked multi-head attention, decoder, autoregressive, attention mask, future token restriction, negative infinity, softmax, attention scores |
Masked multi-head attention is a variant of multi-head attention used in the decoder to ensure that each token only attends to previous tokens and not future ones. This is achieved by applying a mask to the attention scores, setting the values corresponding to future positions to negative infinity before applying softmax. As a result, the model cannot peek at future words when generating a sequence, ensuring that predictions are made in an autoregressive manner.

Encoder-Decoder Attention | encoder-decoder attention, transformer, query, key, value, encoder output, decoder input, token refinement, attention mechanism |
Unlike self-attention, where queries, keys, and values come from the same input sequence, encoder-decoder attention involves queries from the decoder and keys/values from the encoder outputs. This mechanism allows the decoder to incorporate information from the encoder while generating output tokens. The decoder continuously refines its predictions by leveraging the most relevant encoded information through this attention mechanism.

Output: Linear and Softmax Layers | output layer, linear transformation, softmax function, probability distribution, vocabulary vector, token selection, prediction |
Once the decoder has produced a contextual representation of the target sequence, the final step involves mapping this representation to output probabilities. A linear transformation is applied to convert the decoder’s output into a vocabulary-sized vector for each token position. The softmax function is then applied to generate probability distributions over all possible tokens, ensuring that the sum of probabilities equals one. The token with the highest probability is selected as the predicted word for that position, completing the output generation process.

Transformer’s Pipeline | transformer pipeline, input embedding, encoder layer, decoder token generation, learned attention weight, predicted sequence, output refinement |
The transformer’s pipeline begins with input embeddings, which are processed by multiple encoder layers. The encoded representation is then passed to the decoder, which generates tokens one at a time, considering both the encoded input and previously generated tokens. At each decoding step, the model refines its output based on the learned attention weights, gradually forming the final translated or predicted sequence.

