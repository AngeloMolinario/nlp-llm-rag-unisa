Recurrent Neural Network | vanishing gradient, rnn, limitation, short memory|
Neural networks are widely used in text processing, but some architectures face limitations, for example feedforward networks lack memory, they process each input independently without retaining context. This limitation makes handling textual data challenging, as language relies on sequential dependencies. To process text we must provide to the network the entire sequence of words at ones, to do so the entire text must become a single data. This transformation is made using traditional methods like Bag of Words, TF-IDF, and word vector. Recurrent neural networks (RNNs) address the previous limitation by processing sequences iteratively while maintaining a state of previously seen information. Rnns processes sequences of indormation by iterating on the single elements, for example the words of a text represented by their embeddings. RNNs mantains an internal state containing information about what has been seen so far.

RNN structure | rnn, hidden layer, unrolling, loop, cycle|
RNNs is made of an input layer, an hidden layer and the output layer. Each of this layers are feedforward network composed of one or more neurons. The output of the hidded layer is passed to the output layer but it is also fed back as its input along with the normal input received from the input layer. The hidden layer receive as an input the output of the input layer at time t and the output of the input layer at time t+1. This feedback loop enables RNNs to retain context over time, capturing dependencies in sequences. Unrolling an RNN along a timeline expands it into multiple time steps, showing how past information influences future computations. At the initial step, the network processes input without prior context, relying solely on the current data. As the sequence progresses, new inputs are processed alongside the hidden state from previous steps, allowing the network to maintain temporal dependencies. Trainable weights govern how past information influences current computation. While intermediate outputs can be generated at each step, they are often ignored until the final step, where the model produces its output. This allows the network to accumulate relevant information over time, ensuring the final output reflects the entire sequence context.

RNN Training | forward, train, rnn, hidden layer, gradient, vanishing gradient, exploding gradient|
The training process of a recurrent neural network consists of two main phases: the forward pass and backpropagation through time. In the forward pass, the input sequence is processed step by step, with each word passing through a hidden layer that maintains contextual information. At each time step, the hidden layer updates its state and propagates the transformed information forward. Intermediate outputs are generated but often ignored, as the final output is typically the only one used for prediction. The final output is compared with the true label to compute the error. During backpropagation, the error calculated at the final output is propagated backward through time to adjust the network’s weights. This step allows the network to learn from past dependencies, modifying the hidden-to-hidden, input-to-hidden, and hidden-to-output connections. Gradients flow backward across multiple time steps, helping the network improve its ability to capture sequential patterns. However, this process is susceptible to vanishing or exploding gradients, which can impact learning efficiency in long sequences.

RNN Applications | rnn, application, one, many |
Recurrent Neural Networks (RNNs) can be utilized in different ways depending on the structure of input and output sequences. The categorization is based on how many input and output elements are involved in the sequence processing.
The One-to-Many configuration takes a single input token and generates a sequence of outputs. This structure is particularly useful for tasks that require sequence generation from a fixed representation, such as generating chat messages, answering questions, or describing images.
The Many-to-One configuration processes an entire sequence of inputs and condenses it into a single output token. This approach is commonly used in text classification tasks, such as sentiment analysis, language identification, and intent recognition, where a sequence of words is mapped to a single category.
The Many-to-Many configuration takes a sequence of inputs and generates a sequence of outputs. This structure is widely applied in tasks like machine translation, token-level tagging, anonymization, conversational AI, and other applications that require sequence-to-sequence transformations. The input sequence and the output sequence can also have different sizes.
These approaches highlights the flexibility of RNNs in handling various natural language processing and sequence-based tasks, allowing them to model dependencies across time steps in different ways.

RNN for Text Generation | rnn, text generation, coherence|
In text generation tasks, every output at each time step contributes to the final result, making the intermediate outputs just as crucial as the final one. Unlike tasks in which only the last output is relevant, in sequence generation, each predicted word influences the next step, requiring continuous refinement throughout the sequence.
During training, errors are captured and backpropagated at each step, ensuring that the network weights are adjusted incrementally across all time steps. This contrasts with tasks in which only the final step's output is used for error calculation. By summing the errors across all time steps, the model learns to optimize predictions for the entire sequence rather than for a single outcome.
This approach is essential for generating coherent and contextually relevant text, as the network must learn dependencies between previous and future tokens to improve its predictions dynamically.

Bidirectional RNN | variant, rnn, bidirectional rnn |
Bidirectional recurrent neural networks extend standard RNNs by processing sequences in both forward and backward directions. This is achieved by using two recurrent hidden layers, one moving from past to future and another from future to past. The outputs of both layers are concatenated at each time step, allowing the network to capture dependencies that might be overlooked in a unidirectional model. This structure helps in tasks where context from both past and future words is essential for accurate predictions.

LSTM | variant, rnn, lont term, gate, forget, stacked, lstm|
While RNNs theoretically retain information over long sequences, they struggle with long-term dependencies due to the vanishing gradient problem. As layers increase, gradients diminish, making training difficult.
Long Short-Term Memory (LSTM) networks address this issue by introducing a memory state that gets updated at each time step. The architecture includes trainable mechanisms (gates) to control what information to keep and discard, allowing better handling of long range dependencies in sequential data.
Unrolling an LSTM shows how past information is reinjected over time, combating the vanishing gradient problem. Enabling effective learning of long term dependencies, ensuring that context is retained across extended sequences.
Stacked LSTMs improve sequence modeling by adding multiple layers of LSTM units, each refining the output of the previous one. Unlike a single-layer LSTM, where all processing happens in one step, each layer in a stacked architecture has its own memory state, allowing it to capture different levels of abstraction. Lower layers focus on short-term patterns, while deeper layers retain long-term dependencies. This hierarchical structure helps the model understand complex relationships in sequential data, making it more effective for tasks like language modeling and speech recognition.

GRU | variant,rnn,lstm,gru|
Gated Recurrent Units (GRUs) simplify LSTM architecture while still addressing the vanishing gradient problem. Unlike LSTMs, GRUs do not have a separate memory state but instead rely on the hidden state for storing and transferring information across time steps. With fewer parameters than LSTMs, GRUs are computationally more efficient while maintaining comparable performance, particularly in tasks with simpler dependencies. By dynamically adjusting the contribution of past information, GRUs maintain stable gradient flow during backpropagation, improving learning over long sequences without requiring an explicit memory state like LSTMs. Their simpler architecture reduces computational overhead while preserving the ability to model long term dependencies.

Ragged Tensor | tensor, ragged, variable lenght, padding, truncation |
A Ragged Tensor is a type of tensor that allows rows to have variable lengths. It is particularly useful for handling data such as text sequences, where each input may contain a different number of elements, such as sentences with varying word counts. By eliminating the need for padding or truncating sequences to a fixed length, Ragged Tensors improve computational efficiency and reduce overhead by directly managing variable-length data. TensorFlow has supported Ragged Tensors since version 2.0, while PyTorch provides similar functionality through Packed Sequences.

Generative Models | generative, pattern, correct, temperature, prob, deterministic |
Generative models are designed to produce new text by learning patterns and structures from existing corpora. Unlike discriminative models that classify data, generative models create coherent and syntactically correct sequences. While recurrent neural networks can be used for text generation, more advanced architectures like transformers offer improved performance.
Generative models have broad applications, including machine translation, question answering, automatic summarization, text completion, and dialogue systems. They are also useful for creative writing like generating poetry.
Generative models sample from possible outputs rather than always selecting the most probable token. The temperature parameter controls randomness in sampling. Low temperatures ( T<1 ) make predictions more deterministic, while higher values ( T>1 ) encourage diversity and creativity in the generated text.

Language Models | train, language model, statistic, arbitrary|
A language model predicts the probability of the next token given previous ones, capturing the statistical structure of language. Once trained, it can be sampled to generate new text by iteratively predicting and appending tokens to the input. This process allows for generating sequences of arbitrary length.
Training involves feeding a sequence into a recurrent neural network, where each token is processed step by step. The model’s predicted output at each step is compared to the expected token, and errors are used to update network weights through backpropagation. Unlike traditional recurrent neural network training, where errors propagate only at the end, language models apply corrections at each step.

Temperature Scaling
Temperature scaling modifies the probability distribution of token selection in generative models. The formula used to adjust probabilities is:
q_i = exp(log(p_i) / T) ; q'_i = q_i / sum_j q_j
where:
p represents the original probability distribution,
p_i is the probability of token i,
T is the chosen temperature, and it must be positive,
q' is the new probability distribution after applying the temperature scaling.
Lower temperatures make the model more deterministic by concentrating probability mass on higher-likelihood tokens, while higher temperatures flatten the distribution, increasing randomness and creativity in the generated text.